{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91997629",
   "metadata": {},
   "source": [
    "# 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bbe0e",
   "metadata": {},
   "source": [
    "For `Multiple Linear Regression` (with more than one independent variable), the hypothesis function expands to:\n",
    "\n",
    "$y = β₀ + β₁x₁ + β₂x₂ + ... + βₖxₖ$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ x₁, x₂, ..., xₖ $ are the **independent variables**.  \n",
    "- $ β₀ $ is the **intercept**.  \n",
    "- $ β₁, β₂, ..., βₖ $ are the **coefficients**, representing the influence of each respective independent variable on the predicted output.\n",
    "\n",
    "\n",
    "\n",
    "## Multicollinearity\n",
    "\n",
    "Multicollinearity arises when two or more independent variables are highly correlated with each other. This can make it difficult to find the individual contribution of each variable to the dependent variable.\n",
    "\n",
    "To detect multicollinearity we can use:\n",
    "\n",
    "- **Correlation Matrix**: A correlation matrix helps to find relationships between independent variables. High correlations (close to 1 or -1) suggest multicollinearity.\n",
    "- **VIF** (Variance Inflation Factor): VIF quantifies how much the variance of a regression coefficient increases if predictors are correlated. A high VIF typically above 10 indicates multicollinearity.\n",
    "\n",
    "## Assumptions of Multiple Regression Model\n",
    "Similar to simple linear regression we have some assumptions in multiple linear regression which are as follows:\n",
    "\n",
    "1. Linearity: Relationship between dependent and independent variables should be linear.\n",
    "2. Homoscedasticity: Variance of errors should remain constant across all levels of independent variables.\n",
    "3. Multivariate Normality: Residuals should follow a normal distribution.\n",
    "4. No Multicollinearity: Independent variables should not be highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2d63a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Math Way: Least-Squares Estimation\n",
    "\n",
    "The hypothesis function is: $y = β₀ + β₁x₁ + β₂x₂ + ... + βₖxₖ$\n",
    "\n",
    "Assuming that the independent variables are: $\\vec{x_i} = [x₁^i, x₂^i, ..., x_m^i]$\n",
    "\n",
    "and the model's parameters are: $\\vec{β} = [β₀, β₁, ..., β_m]$\n",
    "\n",
    "then the model's prediction would be: $yᵢ ≈ β₀ + \\sum_{j=1}^{m} βⱼ × xⱼ^i$\n",
    "\n",
    "If $\\vec{x_i}$ is extended to  $\\vec{x_i} = [1, x_1^i, x_2^i, \\ldots, x_m^i]$\n",
    "\n",
    "then $y_i$ would become a dot product of the parameter and the independent vectors: $y_i \\approx \\sum_{j=0}^{m} \\beta_j \\times x_j^i = \\vec{\\beta} \\cdot \\vec{x_i}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e861af0",
   "metadata": {},
   "source": [
    "The goal is to **minimize the sum of mean squared loss**: $\\hat{\\vec{\\beta}} = \\arg\\min_{\\vec{\\beta}} L(\\vec{\\beta}) = \\arg\\min_{\\vec{\\beta}} \\sum_{i=1}^{n} (\\vec{\\beta} \\cdot \\vec{x_i} - y_i)^2$\n",
    "\n",
    "Now, putting the independent and dependent variables in matrices $X$ and $Y$ respectively, the loss function can be rewritten as:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "L(\\vec{\\beta}) &= \\|X\\vec{\\beta} - Y\\|^2 \\\\\n",
    "&= (X\\vec{\\beta} - Y)^{T}(X\\vec{\\beta} - Y) \\\\\n",
    "&= Y^{T}Y - Y^{T}X\\vec{\\beta} - \\vec{\\beta}^{T}X^{T}Y + \\vec{\\beta}^{T}X^{T}X\\vec{\\beta}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "As the loss function is convex, the optimum solution lies where the gradient equals zero.  \n",
    "The gradient of the loss function is:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L(\\vec{\\beta})}{\\partial \\vec{\\beta}}\n",
    "&= \\frac{\\partial (Y^{T}Y - Y^{T}X\\vec{\\beta} - \\vec{\\beta}^{T}X^{T}Y + \\vec{\\beta}^{T}X^{T}X\\vec{\\beta})}{\\partial \\vec{\\beta}} \\\\\n",
    "&= -2X^{T}Y + 2X^{T}X\\vec{\\beta}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Setting the gradient to zero produces the optimum parameter:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "-2X^{T}Y + 2X^{T}X\\vec{\\beta} &= 0 \\\\\n",
    "\\Rightarrow X^{T}X\\vec{\\beta} &= X^{T}Y \\\\\n",
    "\\Rightarrow \\hat{\\vec{\\beta}} &= (X^{T}X)^{-1}X^{T}Y\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "**Note:**  \n",
    "The $\\hat{\\beta}$ obtained may indeed be a local minimum; to confirm, one must differentiate once more to obtain the Hessian matrix and verify that it is **positive definite**.  \n",
    "This condition is guaranteed by the **Gauss–Markov theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d69cf",
   "metadata": {},
   "source": [
    "RESUME: https://www.geeksforgeeks.org/machine-learning/ml-multiple-linear-regression-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf35c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
