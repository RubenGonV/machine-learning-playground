## ğŸ“˜ Proyecto 1: RegresiÃ³n Lineal

### ğŸ§  DescripciÃ³n

Este repositorio contiene una implementaciÃ³n **desde cero** del algoritmo de **RegresiÃ³n Lineal**, desarrollada en un Jupyter Notebook.
El objetivo es comprender el funcionamiento interno del modelo **sin depender de librerÃ­as externas** como scikit-learn.
Se cubren los fundamentos teÃ³ricos, la deducciÃ³n matemÃ¡tica y una implementaciÃ³n prÃ¡ctica paso a paso.

La `regresiÃ³n lineal` es un mÃ©todo estadÃ­stico utilizado para **predecir** una variable dependiente continua (variable objetivo) a partir de una o mÃ¡s variables independientes.

#### Â¿Por quÃ© es importante la RegresiÃ³n Lineal?

* **Simplicidad e interpretabilidad:** Es fÃ¡cil de entender e interpretar, por lo que suele ser el punto de partida para aprender sobre aprendizaje automÃ¡tico.
* **Base de otros modelos:** Muchos algoritmos mÃ¡s avanzados, como la regresiÃ³n logÃ­stica o las redes neuronales, se construyen sobre los conceptos de la regresiÃ³n lineal.

#### Supuestos de la RegresiÃ³n Lineal

1. **Linealidad:** La relaciÃ³n entre las variables de entrada (X) y la salida (Y) debe ser una lÃ­nea recta.
2. **Independencia de los errores:** Los errores en las predicciones no deben influirse entre sÃ­.
3. **Varianza constante (Homoscedasticidad):** Los errores deben tener una dispersiÃ³n uniforme a lo largo de todos los valores de entrada.
   Si la dispersiÃ³n cambia (por ejemplo, se abre o se estrecha), se denomina **heteroscedasticidad**, y representa un problema para el modelo.

![homoscedasticity.png](attachment\:homoscedasticity.png)

4. **Normalidad de los errores:** Los errores deben seguir una distribuciÃ³n normal (en forma de campana).
5. **No multicolinealidad (en regresiÃ³n mÃºltiple):** Las variables de entrada no deben estar demasiado correlacionadas entre sÃ­.
6. **No autocorrelaciÃ³n:** Los errores no deben mostrar patrones repetitivos, especialmente en datos temporales.
7. **Aditividad:** El efecto total sobre Y es la suma de los efectos individuales de cada X, sin interacciones entre ellas.

---

### ğŸ“Š Contenido del Notebook

1. **IntroducciÃ³n teÃ³rica a la regresiÃ³n lineal.**
2. **RegresiÃ³n Lineal Simple:**

   Se resuelve utilizando **tres mÃ©todos diferentes**, explicando cÃ³mo funciona cada uno, sus ventajas y desventajas,
   junto con una **representaciÃ³n grÃ¡fica** de cada caso.

   * MÃ­nimos Cuadrados (mÃ©todo estadÃ­stico)
   * EcuaciÃ³n Normal (mÃ©todo algebraico)
   * Descenso del Gradiente (mÃ©todo de aprendizaje automÃ¡tico)

---

### âš™ï¸ TecnologÃ­as Utilizadas

* **Python 3.11+**
* **NumPy**
* **Matplotlib**
* **Jupyter Notebook**

---

### ğŸ§© Contenidos

```
â”œâ”€â”€ simple_linear_regression.ipynb     # Notebook principal con teorÃ­a y cÃ³digo -> Idioma: InglÃ©s
â”œâ”€â”€ README.md                          # Archivo principal
â”œâ”€â”€ README_ES.md                       # Este archivo (versiÃ³n en espaÃ±ol)
```

---

### ğŸ“š Referencias

> * [https://www.geeksforgeeks.org/machine-learning/linear-regression-python-implementation](https://www.geeksforgeeks.org/machine-learning/linear-regression-python-implementation)
> * [https://www.geeksforgeeks.org/machine-learning/ml-linear-regression](https://www.geeksforgeeks.org/machine-learning/ml-linear-regression)
> * [https://www.geeksforgeeks.org/machine-learning/ml-normal-equation-in-linear-regression](https://www.geeksforgeeks.org/machine-learning/ml-normal-equation-in-linear-regression)
> * [https://www.geeksforgeeks.org/machine-learning/gradient-descent-in-linear-regression](https://www.geeksforgeeks.org/machine-learning/gradient-descent-in-linear-regression)

---

### ğŸ‘¨â€ğŸ’» **Autor** : [@RubenGonV](https://github.com/RubenGonV)

> ğŸ“¬ Si este proyecto te resultÃ³ Ãºtil, considera dejar una â­ en el repositorio â€” Â¡motiva mÃ¡s de lo que parece!
